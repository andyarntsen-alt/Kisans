import type { Guide } from './types';

export const supabaseAi: Guide = {
    slug: 'supabase-ai',
    tag: 'GUIDE \u00B7 SUPABASE',
    title: 'Supabase for AI: Vektorer, RAG og backend',
    description: 'Lær hvordan du bruker Supabase som backend for AI-applikasjoner. Fra pgvector og vektorsøk til Edge Functions, RAG-pipelines og Row Level Security. En komplett guide på norsk.',
    keywords: ['supabase', 'supabase ai', 'pgvector', 'vektordatabase', 'rag database', 'supabase norsk', 'supabase edge functions'],
    publishedDate: '2026-02-10',
    relatedSlugs: ['rag-forklart', 'bygg-ai-chatbot', 'openai-api'],
    category: 'bygg-med-ai',
    sections: [
        {
            heading: 'Hva er Supabase?',
            content: 'Supabase er et open source-alternativ til Firebase som gir deg en komplett backend på minutter. I stedet for Googles proprietære tjenester bygger Supabase på PostgreSQL, verdens mest avanserte open source-database. Du får en hosted database, autentisering, fillagring, sanntidsabonnementer og Edge Functions i en og samme plattform. Alt er tilgjengelig på supabase.com, og du kan komme i gang gratis.\n\nDet som gjør Supabase spesielt interessant for AI-utviklere er at PostgreSQL har en moden utvidelse kalt pgvector. Den lar deg lagre og søke i vektorembeddings direkte i databasen, uten å trenge en separat vektordatabase. Det betyr at du kan ha brukertabeller, applikasjonsdata og vektorsøk i ett og samme system, med samme autentisering og tilgangskontroll.\n\nSupabase har vokst raskt siden lanseringen og brukes nå av hundretusenvis av utviklere verden over. Prosjektet er open source under Apache 2.0-lisensen, noe som betyr at du kan selvhoste hele stacken hvis du vil. For de fleste er den hostede tjenesten enklest, men det er betryggende å vite at du ikke er låst til en leverandør. Du eier dataene dine og kan flytte dem når som helst.'
        },
        {
            heading: 'pgvector: Vektorsok i PostgreSQL',
            content: 'For å forstå pgvector må du først forstå hva vektorer er i AI-sammenheng. Når du sender en tekst gjennom en embedding-modell som OpenAIs text-embedding-3-small, får du tilbake en liste med tall, en vektor, som representerer tekstens betydning. To tekster som handler om lignende ting vil ha vektorer som ligger nær hverandre i det matematiske rommet. Dette gjør det mulig å søke etter mening i stedet for nøkkelord.\n\npgvector er en PostgreSQL-utvidelse som legger til en egen vektordatatype og funksjoner for likhetssøk. Du aktiverer den med en enkel SQL-kommando i Supabase: create extension vector. Deretter kan du opprette kolonner av typen vector(1536), der 1536 er dimensjonen til OpenAIs embeddings. Du lagrer vektorer som vanlige rader i tabellen, og kan kjøre likhetssøk med operatorer som cosine distance eller inner product.\n\nFor å gjøre søkene raske støtter pgvector to typer indekser: HNSW og IVFFlat. HNSW (Hierarchical Navigable Small World) er den anbefalte standarden for de fleste bruksområder. Den gir rask og nøyaktig søking og bygger indeksen inkrementelt etter hvert som du setter inn data. IVFFlat kan være mer minneeffektiv for svært store datasett, men krever at du bygger indeksen etter at dataene er lastet inn. For de fleste AI-prosjekter er HNSW det riktige valget.\n\nEn stor fordel med pgvector i Supabase er at vektorsøk og vanlige SQL-spørringer fungerer sammen. Du kan kombinere et semantisk vektorsøk med filtrering på metadata, for eksempel finne de mest relevante dokumentene innenfor en bestemt kategori eller tidsperiode. Du slipper å synkronisere data mellom en relasjonsdatabase og en separat vektordatabase, noe som reduserer kompleksitet og feilkilder betraktelig.'
        },
        {
            heading: 'Bygge en RAG-pipeline med Supabase',
            content: 'RAG (Retrieval-Augmented Generation) handler om å gi AI-modellen tilgang til dine egne data slik at den kan gi presise svar. Med Supabase som backend har du alt du trenger for en komplett RAG-pipeline. Prosessen har tre hovedfaser: forberedelse av dokumenter, lagring av embeddings og henting ved spørretidspunkt.\n\nForberedelsen starter med chunking. Du deler dokumentene dine opp i mindre biter, typisk 500 til 1000 tokens per chunk. Biter som er for store gir upresise søkeresultater, mens biter som er for små mister kontekst. En god praksis er å dele etter naturlige grenser som avsnitt eller seksjoner, med litt overlapp mellom bitene slik at kontekst ikke går tapt på grensene. Deretter sender du hver bit gjennom en embedding-modell og lagrer både teksten og vektoren i en Supabase-tabell.\n\nNår en bruker stiller et spørsmål, konverterer du spørsmålet til en vektor med den samme embedding-modellen. Deretter kjører du et vektorsøk i Supabase for å finne de mest relevante dokumentbitene. En typisk Supabase-funksjon for dette bruker match_documents som tar inn spørsmålsvektoren og returnerer de N mest like radene. Du sender de hentede tekstbitene som kontekst til AI-modellen sammen med brukerens spørsmål, og modellen genererer et svar basert på den faktiske informasjonen.\n\nSupabase gjør det enkelt å bygge dette som en database-funksjon i PostgreSQL. Du oppretter en stored procedure som tar inn en spørsmålsvektor, kjører et cosine similarity-søk, og returnerer de mest relevante resultatene. Fra klienten kaller du denne funksjonen via Supabase sin RPC-metode. Hele flyten fra spørsmål til svar kan bygges på under hundre linjer kode, og du får et system som skalerer godt fordi PostgreSQL håndterer indeksering og søkeoptimalisering for deg.'
        },
        {
            heading: 'Edge Functions for AI-logikk',
            content: 'Supabase Edge Functions er serverless-funksjoner som kjører på Deno-runtimen, nær brukerne dine over hele verden. De er perfekte for AI-logikk fordi du kan kalle eksterne AI-API-er, prosessere data og returnere resultater uten å sette opp en egen server. Funksjonene skrives i TypeScript og deployes med en enkel CLI-kommando.\n\nFor AI-applikasjoner bruker du Edge Functions typisk til å orkestrere hele RAG-flyten. Funksjonen tar imot brukerens spørsmål, kaller OpenAI for å generere en embedding-vektor, kjører et vektorsøk mot Supabase-databasen, setter sammen konteksten, og sender alt til en AI-modell for å generere svaret. Du kan også bruke Edge Functions til å strømme svar tilbake til klienten i sanntid, noe som gir en mye bedre brukeropplevelse for chatboter.\n\nEn praktisk fordel med Edge Functions er at API-nøklene dine aldri eksponeres til klienten. All kommunikasjon med OpenAI, Anthropic eller andre AI-tjenester skjer på serversiden. Du lagrer nøklene som miljøvariabler i Supabase-dashbordet, og Edge Functions har tilgang til dem uten at de noensinne sendes til nettleseren. Funksjonene har også innebygd tilgang til Supabase-klienten, så du trenger ikke konfigurere databasetilkoblinger manuelt.\n\nEdge Functions har en generøs grense på gratisnivået med 500 000 invoksjoner per måned og støtter opptil 150 MB minnebruk per funksjon. For de fleste AI-prosjekter i oppstartsfasen er dette mer enn nok. Funksjoner kan også trigges av database-webhooks, cron-jobber eller HTTP-forespørsler, noe som gjør det fleksibelt å bygge bakgrunnsjobber som prosesserer og indekserer nye dokumenter automatisk.'
        },
        {
            heading: 'Autentisering og Row Level Security',
            content: 'Når du bygger en AI-applikasjon som håndterer sensitive data, er sikkerhet like viktig som funksjonaliteten. Supabase har innebygd autentisering som støtter e-post og passord, magiske lenker, OAuth-innlogging via Google, GitHub og andre leverandører, og tofaktorautentisering. Du trenger ikke en tredjepartstjeneste for pålogging, og alt integrerer sømløst med databasen.\n\nRow Level Security (RLS) er PostgreSQL-funksjonalitet som lar deg definere tilgangsregler direkte på databasenivå. I en AI-applikasjon der brukere laster opp egne dokumenter, kan du sette opp RLS-policyer slik at hver bruker kun kan lese og søke i sine egne embeddings. Selv om noen skulle finne en feil i klienten, kan de ikke hente andre brukeres data fordi databasen selv håndhever tilgangskontrollen. Du skriver policyer som «brukere kan bare lese rader der user_id matcher deres egen innlogging», og PostgreSQL filtrerer automatisk.\n\nFor AI-prosjekter med vektorsøk betyr dette at du trygt kan ha en flerbrukerdatabase der alle embeddings ligger i samme tabell. RLS sørger for at vektorsøk automatisk filtreres per bruker. Når bruker A gjør et semantisk søk, returneres bare resultater fra bruker As dokumenter, selv om bruker B har lignende innhold i databasen. Dette er en elegant løsning som sparer deg for kompleksiteten med separate databaser eller manuell filtrering i applikasjonskoden.\n\nKombinasjonen av autentisering og RLS gjør det enkelt å bygge SaaS-produkter med AI-funksjonalitet. Du kan la brukere laste opp dokumenter, bygge egne kunnskapsbaser og chatte med sine data, alt mens du er trygg på at dataene er isolert mellom brukere. Supabase gir deg dette ut av boksen uten at du trenger å bygge et eget tilgangskontrollsystem.'
        },
        {
            heading: 'Priser og gratisniva',
            content: 'Supabase har et sjenerøst gratisnivå som gjør det mulig å bygge og teste AI-prosjekter uten å betale en krone. Gratisplanen inkluderer to aktive prosjekter, 500 MB databaselagring, 1 GB fillagring, 5 GB båndbredde per måned og 500 000 Edge Function-invoksjoner. Du får også 50 000 månedlige aktive brukere for autentisering. For hobbyprosjekter, prototyper og tidlige oppstartsprosjekter er dette rikelig.\n\nNår prosjektet ditt vokser, er Pro-planen neste steg til 25 dollar per måned per prosjekt. Den gir deg 8 GB databaselagring, 100 GB fillagring, 250 GB båndbredde og 2 millioner Edge Function-invoksjoner. Pro-planen fjerner også den automatiske pauseringen av inaktive prosjekter som gjelder for gratisplanen, og gir tilgang til daglige backups og e-poststøtte. For AI-applikasjoner i produksjon med ekte brukere er Pro-planen et naturlig valg.\n\nFor prosjekter som trenger mer ytelse, tilbyr Supabase compute add-ons som lar deg skalere databaseinstansen. Du kan oppgradere fra den delte instansen til dedikerte instanser med mer RAM og CPU-kraft. Dette er spesielt relevant for AI-prosjekter med store vektortabeller, fordi pgvector-søk drar nytte av mer tilgjengelig minne. HNSW-indekser lastes inn i minnet for rask søking, så større instanser gir raskere vektorsøk.\n\nSammenlignet med dedikerte vektordatabaser som Pinecone er Supabase ofte mer kostnadseffektivt for prosjekter der du uansett trenger en relasjonsdatabase. I stedet for å betale separat for database og vektorsøk, får du begge deler i ett abonnement. For mindre og mellomstore prosjekter med opptil noen hundre tusen vektorer er Supabase med pgvector et solid og rimelig valg. For ekstremt store datasett med millioner av vektorer og krav til ultra-lav latens kan en spesialisert vektordatabase fortsatt være verdt å vurdere.'
        }
    ]
};
